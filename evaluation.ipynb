{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeae4e41-781c-4903-877b-82eea4f450c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_model\", local_files_only=True)\n",
    "\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"./sentiment_model\", local_files_only=True)\n",
    "\n",
    "stars_model = AutoModelForSequenceClassification.from_pretrained(\"./stars_model\", local_files_only=True)\n",
    "\n",
    "\n",
    "def preprocess_imdb(dataset_dir):\n",
    "    cur_dir = pathlib.Path(dataset_dir)\n",
    "    texts = []\n",
    "    sentiments = []\n",
    "    stars = []\n",
    "    for sentiment_dir in ['pos', 'neg']:\n",
    "        for text_file in (cur_dir/sentiment_dir).iterdir():\n",
    "            texts.append(text_file.read_text(encoding='utf-8'))\n",
    "            sentiments.append(1 if sentiment_dir == 'pos' else 0)\n",
    "            star_count = int(text_file.name[:-4].split('_')[1])\n",
    "            stars.append(star_count - 1)\n",
    "    \n",
    "    return texts, sentiments, stars\n",
    "\n",
    "\n",
    "texts_train, sentiments_train, stars_train = preprocess_imdb('aclImdb/train')\n",
    "texts_test, sentiments_test, stars_test = preprocess_imdb('aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5e7bb9-286f-4a2a-820b-7f87abaa7bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(texts_train, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=256)\n",
    "test_encodings = tokenizer(texts_test, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=256)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"],\n",
    "                                   \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "                                   \"labels\": torch.tensor(sentiments_train)})\n",
    "train_dataset.set_format(\"pt\")\n",
    "\n",
    "\n",
    "sentiments_test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"],\n",
    "                                  \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "                                  \"labels\": torch.tensor(sentiments_test)})\n",
    "sentiments_test_dataset.set_format(\"pt\")\n",
    "\n",
    "stars_test_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"],\n",
    "                                  \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "                                  \"labels\": torch.tensor(stars_test)})\n",
    "stars_test_dataset.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb57585-8870-466c-ab09-2099462a5030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d74498-5ddf-4537-afa2-05d7978f3308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-2.1877115 ,  2.2571092 ],\n",
      "       [-1.7224199 ,  1.685449  ],\n",
      "       [-1.9839073 ,  1.8819491 ],\n",
      "       ...,\n",
      "       [ 1.4794309 , -1.9533433 ],\n",
      "       [ 1.8856187 , -1.955772  ],\n",
      "       [ 0.31323573, -1.0763208 ]], dtype=float32), label_ids=array([1, 1, 1, ..., 0, 0, 0], dtype=int64), metrics={'test_loss': 0.21519549190998077, 'test_runtime': 158.1381, 'test_samples_per_second': 158.09, 'test_steps_per_second': 4.945})\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='tmp',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "sentiment_trainer = Trainer(\n",
    "    model=sentiment_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "sentiment_res = sentiment_trainer.predict(sentiments_test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3355106e-7e4e-4688-8db7-471f70bd79ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Model - Test Accuracy: 0.91824\n",
      "Sentiment Model - Test F1: 0.9185462660396907\n"
     ]
    }
   ],
   "source": [
    "sentiment_preds = np.argmax(sentiment_res.predictions, axis=-1)\n",
    "\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "sentiment_accuracy = (np.array(sentiment_preds) == np.array(sentiments_test)).mean()\n",
    "sentiment_f1 = f1_score(sentiments_test, sentiment_preds)\n",
    "\n",
    "print(\"Sentiment Model - Test Accuracy:\", sentiment_accuracy)\n",
    "print(\"Sentiment Model - Test F1:\", sentiment_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b7b0b06-e5d9-454b-b4f0-8ef6959182b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stars_trainer = Trainer(\n",
    "    model=stars_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "res = stars_trainer.predict(stars_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3de03142-dd0b-44f8-9be5-5ac782f8ebe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stars Model - Test Accuracy: 0.46212\n",
      "Stars Model - Test F1: 0.46212\n",
      "Stars Model - Test MAE: 1.15804\n"
     ]
    }
   ],
   "source": [
    "stars_preds = np.argmax(res.predictions, axis=-1)\n",
    "\n",
    "stars_accuracy = (np.array(stars_preds) == np.array(stars_test)).mean()\n",
    "stars_f1 = f1_score(stars_test, stars_preds, average='micro')\n",
    "stars_mae = mean_absolute_error(stars_test, stars_preds)\n",
    "\n",
    "print(\"Stars Model - Test Accuracy:\", stars_accuracy)\n",
    "print(\"Stars Model - Test F1:\", stars_f1)\n",
    "print(\"Stars Model - Test MAE:\", stars_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec16d53f-f6b4-490f-8c17-5831a14a1e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70704\n"
     ]
    }
   ],
   "source": [
    "stars_one_off_accuracy = (np.abs(np.array(preds) - np.array(stars_test)) <= 1).mean()\n",
    "print(stars_one_off_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8436bc7-08a2-4a60-a864-815da2f384ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
